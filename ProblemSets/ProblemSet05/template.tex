\documentclass{report}

\input{preamble}
\input{macros}
\input{letterfonts}

\title{\Huge{Math 2700.009}\\Problem Set 5}
\author{\huge{Ezekiel Berumen}}
\date{20 February 2024}

\begin{document}

\maketitle
\newpage

\qs{}{Show directly that the following vectors constitute a basis for $\mathbb{R}^4$, do not appeal to the dimension of $\mathbb{R}^4$ to do so, show that they are both linearly independent and span $\mathbb{R}^4$.
$$
\left[\begin{array}{l}
1 \\
2 \\
3 \\
4
\end{array}\right],\left[\begin{array}{c}
0 \\
-1 \\
0 \\
-2
\end{array}\right],\left[\begin{array}{l}
1 \\
0 \\
1 \\
0
\end{array}\right],\left[\begin{array}{c}
3 \\
-1 \\
7 \\
0
\end{array}\right]
$$}
\sol In order to show that the set of vectors is a basis of $\mathbb{R}^4$,  I will show that the vectors span $\mathbb{R}^4$. In doing so it will also be proved that the vectors are linearly indepdent.  To do so we must show that a given vector say
$$
\vec{v} =
\begin{bmatrix}
x_1 \\ x_2 \\ x_3 \\ x_4
\end{bmatrix}
\in \mathbb{R}^4
$$
can be represented as some linear combination of the four vectors that we are given.  That is to to say that there exists some scalars $c_1, c_2,c_3,c_4\in\mathbb{R}$ such that
$$
c_1 \begin{bmatrix} 1 \\ 2 \\ 3 \\ 4 \end{bmatrix} + c_2 \begin{bmatrix} 0 \\ -1 \\ 0 \\ -2 \end{bmatrix}
+ c_3 \begin{bmatrix} 1 \\ 0 \\ 1 \\ 0 \end{bmatrix} + c_4 \begin{bmatrix} 3 \\ -1 \\ 7 \\ 0 \end{bmatrix}
= \begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \end{bmatrix}
$$
This implies that it is possible to set up a system of equations,  and further an augmented matrix to then perform row reduction operations to find whether a consistent solution exists for the system.  In this case,  a consistent solution will imply that there exists scalars such that any arbitrary vector from $\mathbb{R}^4$ can be found in the span of the vectors.  See:
$$
\begin{aligned}
c_1 + 0c_2 + c_3 + 3c_4 & = x_1 \\
2c_1 - c_2 + 0c_3 - c_4 &  = x_2 \\
3c_1 + 0c_2 + 1c_3 + 7c_4 & = x_3 \\
4c_1 - 2c_2 + 0c_3 + 0c_4 & = x_4 \\
\end{aligned}
$$
$$
\begin{array}{cccc|c}
    1 & 0 & 1 & 3 & x_1 \\
    2 & -1 & 0 & -1 & x_2 \\
    3 & 0 & 1 & 7 & x_3 \\
    4 & -2 & 0 & 0 & x_4 \\
\end{array}
$$
The augmented matrix can now be row reduced to determine whether it is consistent.  See:
\begin{align*}
	&\begin{array}{cccc|c}
		1 & 0 & 1 & 3 & x_1 \\
		0 & -1 & -2 & -7 & -2x_1 + x_2 \\
		0 & 0 & -2 & -2 & -3x_1 + x_3 \\
		0 & -2 & -4 & -12 & -4x_1 + x_4
	\end{array}
	&& \begin{aligned} & R_2 - 2R_1 \rightarrow R_2 \\ & R_3 - 3R_1 \rightarrow R_3 \\ & R_4 - 4R_1 
	\rightarrow R_4 \end{aligned}
	&\begin{array}{cccc|c}
		1 & 0 & 1 & 3 & x_1 \\
		0 & 1 & 2 & 7 & 2x_1 - x_2 \\
		0 & 0 & -2 & -2 & -3x_1 + x_3 \\
		0 & -2 & -4 & -12 & -4x_1 + x_4
	\end{array}
	&& -R_2 \rightarrow R_2 \\
	&\begin{array}{cccc|c}
		1 & 0 & 1 & 3 &  x_1 \\
		0 & 1 & 2 & 7 & 2x_1 - x_2 \\
		0 & 0 & -2 & -2 & -3x_1 + x_3 \\
		0 & 0 & 0 & 2 & -2x_2 + x_4
	\end{array}
	&& R_4 + 2R_2 \rightarrow R_4
	&\begin{array}{cccc|c}
		1 & 0 & 1 & 2 &  x_1 \\
		0 & 1 & 2 & 7 & 2x_1 - x_2 \\
		0 & 0 & 1 & 1 & \frac{3x_1 - x_3}{2} \\
		0 & 0 & 0 & 2 & -2x_2 + x_4
	\end{array}
	&& -\frac{1}{2}R_3 \rightarrow R_3 \\
	&\begin{array}{cccc|c}
		1 & 0 & 0 & 2 &  -\frac{x_1 -x_3}{2}\\
		0 & 1 & 0 & 5 & -x_1-x_2+x_3\\
		0 & 0 & 1 & 1 & \frac{3x_1 - x_3}{2} \\
		0 & 0 & 0 & 2 & -2x_2 + x_4
	\end{array}
	&& \begin{aligned} & R_1 - R_3 \rightarrow R_1 \\ & R_2 - 2R_3 \rightarrow R_2\end{aligned}
	&\begin{array}{cccc|c}
		1 & 0 & 0 & 0 & -\frac{x_1 - x_3}{2} + 2x_2 -x_4 \\
		0 & 1 & 0 & 1 & -x_1 + x_2 + x_3 + x_4 \\
		0 & 0 & 1 & 0 & \frac{3x_1 + x_2 - x_3 - x_4}{2} \\
		0 & 0 & 0 & 2 & -2x_2 + x_4
	\end{array}
	&& \begin{aligned} & R_1 - R_4 \rightarrow R_1 \\ & R2 - 2R_4 \rightarrow R_2 \\ & R_3 - \frac{1}{2}R_4 
	\rightarrow R_3\end{aligned} \\
	&\begin{array}{cccc|c}
		1 & 0 & 0 & 0 & -\frac{x_1 - x_3}{2} + 2x_2 -x_4 \\
		0 & 1 & 0 & 1 & -x_1 + x_2 + x_3 + x_4 \\
		0 & 0 & 1 & 0 & \frac{3x_1 + x_2 - x_3 - x_4}{2} \\
		0 & 0 & 0 & 1 & -x_2 + \frac{x_4}{2}
	\end{array}
	&& \frac{1}{2}R_4 \rightarrow R_4
	&\begin{array}{cccc|c}
		1 & 0 & 0 & 0 & -\frac{x_1 - x_3}{2} + 2x_2 -x_4 \\
		0 & 1 & 0 & 0 & -x_1 + 2x_2 + x_3 + \frac{x_4}{2} \\
		0 & 0 & 1 & 0 & \frac{3x_1 + x_2 - x_3 - x_4}{2} \\
		0 & 0 & 0 & 1 & -x_2 + \frac{x_4}{2}
	\end{array}
	&& R_2 - R_4 \rightarrow R_4
\end{align*}
Although the constants in the augmented matrix appear to be a bit complex, they are not necessarily important,  since they represent values in $\mathbb{R}$.  The main idea when analyzing this RREF augmented matrix is that it is max rank, that is to say that the system is consistent.  In the context of this question,  this implies that the vectors are linearly independent.  Since the augmented matrix is consistent,  it suffices to say that it is possible to generate any vector in the space $\mathbb{R}^4$ and thus the vectors span $\mathbb{R}^4$.  Since the vectors are both linearly independent and span $\mathbb{R}^4$,  it can be said that the vectors are a basis for $\mathbb{R}^4$.
\qs{}{Are the following vectors linearly independent? Do they span $\mathbb{R}^3$ ? Justify your answers.
$$
\left[\begin{array}{c}
1 \\
-1 \\
2
\end{array}\right],\left[\begin{array}{c}
-2 \\
1 \\
-1
\end{array}\right],\left[\begin{array}{c}
0 \\
-1 \\
3
\end{array}\right],\left[\begin{array}{c}
-1 \\
-1 \\
4
\end{array}\right]
$$}
\sol The vectors are not linearly independent.  This is because there exists a vector in this set of vectors that can be expressed as a combination of other vectors in this set, in particular we can see that:
$$
\begin{bmatrix} 0 \\ -1 \\ 3 \end{bmatrix} =
2\begin{bmatrix} 1 \\ -1 \\ 2 \end{bmatrix} +
\begin{bmatrix} -2 \\ 1 \\ -1 \end{bmatrix}
$$
Further, this new set of vectors also includes a linearly dependent vector, since we can see that
$$
\begin{aligned}
	3\begin{bmatrix} 1 \\ -1 \\ 2 \end{bmatrix} +
	2\begin{bmatrix} -2 \\ 1 \\ -1 \end{bmatrix} & =
	\begin{bmatrix} 3 \\ -3 \\ 6 \end{bmatrix} +
	\begin{bmatrix} -4 \\ 2 \\ -2 \end{bmatrix} \\
	& =
	\begin{bmatrix} 3 - 4 \\ -3 + 2 \\ 6 - 2 \end{bmatrix} \\
	& =
	\begin{bmatrix} -1 \\ -1 \\ 4 \end{bmatrix}
\end{aligned}
$$
Thus we can define a new set of vectors called $S'$ to be defined by:
$$
S' = 
\left\{
\left[\begin{array}{c}
1 \\
-1 \\
2
\end{array}\right],\left[\begin{array}{c}
-2 \\
1 \\
-1
\end{array}\right]
\right\}
$$
Whose span will match the span of the original set of vectors, since the newly generated set $S'$ contains only linearly independent vectors. Appealing to the definition of dimension, it is easy to tell that this set of vectors does not span $\mathbb{R}^3$. That is because the $dim(\mathbb{R}^3)=3$, and for $S'$ to be a spanning set, then it must have 3 vectors, all of which must be linearly independent. In this case since $S'$ only has two linearly independent vectors, then it cannot be a spanning set and thus since $span(S') =$ the span of the 4 original vectors, they do not span $\mathbb{R}^3$.
\newpage
\qs{}{What is the dimension of the following subspace of $\mathbb{R}^3$ ? Justify your answer.
$$
\operatorname{span}\left(\left[\begin{array}{c}
1 \\
-1 \\
2
\end{array}\right],\left[\begin{array}{l}
0 \\
3 \\
4
\end{array}\right],\left[\begin{array}{l}
2 \\
1 \\
8
\end{array}\right]\right)
$$}
\sol The dimension of the subspace generated by taking the span of the vectors is 2. This is because one of the vectors in the set is linearly dependent, in particular we can see that
$$
\begin{aligned}
	2 \begin{bmatrix} 1 \\ -1 \\ 2 \end{bmatrix} +
	\begin{bmatrix} 0 \\ 3 \\ 4 \end{bmatrix}  & =
	\begin{bmatrix} 2 \\ -2 \\ 4 \end{bmatrix} +
	\begin{bmatrix} 0 \\ 3 \\ 4 \end{bmatrix} \\
	& = \begin{bmatrix} 2 + 0 \\ -2 + 3 \\ 4 + 4 \end{bmatrix} \\
	& = \begin{bmatrix} 2 \\ 1 \\ 8 \ \end{bmatrix}
\end{aligned}
$$
Because of this, it is sufficient to say that the vectors form a 2-dimensional subspace of $\mathbb{R}^3$. This is because by definition, a basis is a set of linearly independent vectors that span a subspace. Since the two remaining vectors are linearly indepdendent and span the subspace, they serve as a basis for this subspace, and thus the dimension is 2.
\qs{}{Suppose $T: \mathbb{R}^3 \rightarrow \mathbb{R}^2$ was a linear transformation so that
$$
T\left(\left[\begin{array}{c}
1 \\
-1 \\
2
\end{array}\right]\right)=\left[\begin{array}{l}
1 \\
2
\end{array}\right] \quad \text { and } \quad T\left(\left[\begin{array}{l}
0 \\
2 \\
3
\end{array}\right]\right)=\left[\begin{array}{l}
2 \\
3
\end{array}\right] .
$$

What is
$$
T\left(\left[\begin{array}{c}
2 \\
4 \\
13
\end{array}\right]\right) ?
$$}
\begin{note}
If $V, W$ are vector spaces, a linear transformation from $V$ to $W$ is a function $T: V \rightarrow W$ so that: \\
1) if $\vec{x},\vec{y}\in V$ then $T(\vec{x}+\vec{y}) = T(\vec{x}) + T(\vec{y})$ \\
2) if $\vec{x} \in V$ and $c\in\mathbb{R}$ then $T(c\vec{x}) = cT(\vec{x})$
\end{note}
\sol Given that linear transformations have this scalar multiplication property, then we can see that
$$
\begin{aligned}
	T\left(2\begin{bmatrix} 1 \\ -1 \\ 2 \end{bmatrix}\right) & = 2\begin{bmatrix} 1 \\ 2 \end{bmatrix} \\
	T\left(\begin{bmatrix} 2 \\ -2 \\ 4 \end{bmatrix}\right) & = \begin{bmatrix} 2 \\ 4 \end{bmatrix}
\end{aligned}
$$
and also that
$$
\begin{aligned}
	T\left(3\begin{bmatrix} 0 \\ 2 \\ 3 \end{bmatrix}\right) & = 3\begin{bmatrix} 2 \\ 3 \end{bmatrix} \\
	T\left(\begin{bmatrix} 0 \\ 6 \\ 9 \end{bmatrix}\right) & = \begin{bmatrix} 6 \\ 9 \end{bmatrix}
\end{aligned}
$$
This is particularly important because of the additive property of linear transformations, which allows us to see that
$$
\begin{aligned}
	T\left(\begin{bmatrix} 2 \\ -2 \\ 4 \end{bmatrix} + \begin{bmatrix} 0 \\ 6 \\ 9 \end{bmatrix}\right) & =
	T\left(\begin{bmatrix} 2 \\ -2 \\ 4 \end{bmatrix}\right) + T\left(\begin{bmatrix} 0 \\ 6 \\ 9\end{bmatrix}\right) \\
	T\left(\begin{bmatrix} 2 \\ 4 \\ 13 \end{bmatrix}\right) & = \begin{bmatrix} 2 \\ 4 \end{bmatrix} + \begin{bmatrix} 6 \\ 9 \end{bmatrix} \\
	& = \begin{bmatrix} 8 \\ 13 \end{bmatrix}
\end{aligned}
$$
And thus using these properties of linear transformation we find that the value of the unknown entry.
\qs{}{Verify that the following is a linear transformation:
$$
\begin{gathered}
T: C([0,1]) \rightarrow C([0,1]) \\
T(f)=\int x^2 f(x) \mathrm{d} x
\end{gathered}
$$
where the constant of integration is always zero. Would this function be a linear transformation if the constant of integration was one instead?}
\begin{note}
Recall that if $V, W$ are vector spaces, to verify a linear transformation from $V$ to $W$, two properties must be verified: \\
Additivity: A function $T: V \rightarrow W$ is additive if for any $\vec{x},\vec{y}\in V$, $T(\vec{x} + \vec{y}) = T(\vec{x}) + T(\vec{y})$ \\
Homogeneity: A function $T: V \rightarrow W$ is homogeneous if for any $c\in\mathbb{R}$ and $\vec{x}\in V$, $T(c\vec{x}) = cT(\vec{x})$
\end{note}
\sol Firstly, additivity must be verified. That is to say given some $f,g\in C([0,1])$, it must be verifable that $T(f+g) = T(f) + T(g)$. Firstly, evaluating $T(f)+T(g)$:
$$
T(f) + T(g) =  \int{x^2f(x)dx} + \int{x^2g(x)dx}
$$
and then evaluating the equation $T(f+g)$
$$
\begin{aligned}
	T(f+g) & = \int{x^2(f(x) + g(x))\mathrm{d}x} \\
	& = \int{x^2f(x) + x^2g(x)\mathrm{d}x}\quad\text{By distributive property} \\
	& = \int{x^2f(x)\mathrm{d}x} + \int{x^2g(x)\mathrm{d}x}\quad\text{By sum/difference property} \\
	& = T(f) + T(g)
\end{aligned}
$$
So hence the additivity of the function has been verified.
\newpage
\noindent
Next, to verify the linear transformation, homogeneity must also be verified. That is to say given some scalar $c\in\mathbb{R}$ and some $f\in C([0,1])$, it must be verifiable that $T(cf) = cT(f)$. Firstly, evaluating $cT(f)$:
$$
cT(f) = c\int{x^2f(x)\mathrm{d}x}
$$
and then evaluating the equation $T(cf)$:
$$
\begin{aligned}
	T(cf) & = \int{x^2(cf(x))\mathrm{d}x} \\
	& = c\int{x^2f(x)\mathrm{d}x} \quad\text{By constant multiple property} \\
	& = cT(f)
\end{aligned}
$$
So hence, homogeneity of the function has also been verified. Since both additivity and homogeneity have been verified for this function, it is a linear transformation when the constant of integration is always zero. \\

\noindent In order to check if the function is a linear transformation if the constant of integration is always one, then the same properties must be verified. Firstly, to verify additity given this condition, then it must be verifiable that given some $f,g\in C([0,1])$, then $T(f+g) = T(f) + T(g)$. Beginning by evaluating $T(f) + T(g)$:
$$
\begin{aligned}
	T(f) + T(g) & = (\int{x^2f(x)dx} + 1) + (\int{x^2g(x)dx} + 1) \\
	& = \int{x^2f(x)dx} + \int{x^2g(x)dx} + 2
\end{aligned}
$$
and then evaluating the equation $T(f+g)$
$$
\begin{aligned}
	T(f+g) & = \int{x^2(f(x) + g(x))\mathrm{d}x} + 1 \\
	& = \int{x^2f(x) + x^2g(x)\mathrm{d}x} + 1\quad\text{By distributive property} \\
	& = \int{x^2f(x)\mathrm{d}x} + \int{x^2g(x)\mathrm{d}x} + 1\quad\text{By sum/difference property} \\
	& \neq T(f) + T(g)
\end{aligned}
$$
Since the function fails the additive property verification, it cannot be a linear transformation and does not further require the homogeneity verificaiton. Hence the function is not a linear transformation if the constant of integration is not 0.
\qs{}{Argue that if $\mathfrak{B}$ is a basis for a finite dimensional vector space $V$ and $\mathfrak{E}$ is a set of vectors from $V$ of $\operatorname{size} \operatorname{dim}(V)$ with $\mathfrak{B} \subseteq \operatorname{span}(\mathfrak{E})$ then $\mathfrak{E}$ is also a basis for $V$.}
\sol By the definition of basis, if $\mathfrak{B}$ is a basis of a finite dimensional vector space $V$, then $\mathfrak{B}$ is both linearly independent, and also spans $V$. In order to argue that $\mathfrak{E}$ is also a basis for $V$, then it must be shown that $\mathfrak{E}$ satisfies two properties: linearly independenence, and also that it spans $V$. \\

\noindent First, linear independence can be verified. If $\mathfrak{E}$ is a set of vectors of size $\operatorname{dim}(V)$, then it suffices to say that $|\mathfrak{B}| = |\mathfrak{E}|$. This is because the dimension of a vector space is the number of elements in a basis. Then suppose $\mathfrak{E}$ is not linearly independent, that is to say that at least one vector in $\mathfrak{E}$ can be expressed as linear combination of other vectors in the set. However, since $\mathfrak{B}\subseteq\mathfrak{E}$, this would imply that at least one vector in $\mathfrak{B}$ can be expressed as a linear combination of vectors in $\mathfrak{E}$. This is a contradiction because if $\mathfrak{B}$ is a basis, then it must be linearly independent, hence $\mathfrak{E}$ must be linearly independent. \\

\noindent Next, we can check if $\mathfrak{E}$ is a spanning set. Since $\mathfrak{B}\subseteq\mathfrak{E}$, it suffices to say that any vector from $\mathfrak{B}$ can be expressed as some linear combination of vectors in $\mathfrak{E}$. By definition of spanning set, every vector in the vector space $V$ can be expressed as some linear combination of vectors in $\mathfrak{B}$. Because any vector from $\mathfrak{B}$ can be expressed as a linear combination of vectors from $\mathfrak{E}$, it is also sufficient to say that any vector in $V$ can be expressed as some linear combination of vectors in set $\mathfrak{E}$. Hence $\mathfrak{E}$ is a spanning set of vector space $V$. \\

\noindent Because the set of vectors $\mathfrak{E}$ satisfies both of these conditions, then we can say that $\mathfrak{E}$ is also a basis for $V$.
\qs{}{Suppose $W_1$ and $W_2$ are finite dimensional subspaces of a vector space $V$ so that the only vector in both subspaces is the zero vector, that is $W_1 \cap W_2=\{\overrightarrow{0}\}$. Show that
$$
\operatorname{dim}\left(W_1+W_2\right)=\operatorname{dim}\left(W_1\right)+\operatorname{dim}\left(W_2\right)
$$
by taking a basis $v_1, \ldots, v_n$ for $W_1$ and a basis $u_1, \ldots, u_m$ for $W_2$ and arguing that $v_1, \ldots, v_n, u_1, \ldots, u_m$ is a basis for $W_1+W_2$, that is $v_1, \ldots, v_n, u_1, \ldots, u_m$ is both linearly independent and spans $W_1+W_2$.}
\sol If $W_1$ and $W_2$ are finite dimensional subspaces of a vector space $V$ so that the only vector in both subspaces is the zero vector, that means that the subspaces include vectors that are unique to their respective subspace and the vector space $V$. That is to say that if there exist some $\vec{v}\in W_1$ then $\vec{v}\notin W_2$ and conversely, if some $\vec{u}\in W_2$ then $\vec{u}\notin W_1$, so long as $\vec{v}\neq\vec{u}\neq\vec{0}^V$. \\

\noindent With this idea in mind, taking a basis of $W_1$ and a basis of $W_2$ will result in two unique sets of linearly independent vectors that span their respective subspaces. The basis for subspace $W_1$ will have $\operatorname{dim}\left(W_1\right)$, and the basis for subspace $W_2$ will have $\operatorname{dim}\left(W_2\right)$. Because the only vector that is shared between the subspaces is the zero vectors, the span of these vectors in each respective cannot be equal, that is to say $\operatorname{span}(\vec{v_1}, \ldots, \vec{v_n}) \neq \operatorname{span}(\vec{u_1}, \dots, \vec{u_m})$. Since we can take this to be true, then we can also take that a basis for $W_1 + W_2$ can be found by combining the two sets of vectors, i.e., the set of vectors \{$\vec{v_1}, \ldots, \vec{v_n}, \vec{u_1}, \ldots, \vec{u_m}$\} is a basis of $W_1 + W_2$. \\

\noindent This is because if $W_1 + W_2$ is a subspace, then it must be closed under addition. That is to say that any vector $\vec{w}\in W_1 + W_2$ can be expressed as $\vec{w} = \vec{v} + \vec{u}$ where $\vec{v}\in W_1$ and $\vec{u}\in W_2$. Since the set of vectors \{$\vec{v_1}, \dots, \vec{v_n}$\} spans $W_1$, any vector $\vec{v}\in W_1$ can be expressed as a linear combination of vectors in the set. Similarly, since \{$\vec{u_1}, \dots, \vec{u_m}$\} spans $W_2$, any vector $\vec{u}\in W_2$ can be expressed as a linear combination of vectors in the set. Thus any vector $\vec{w}\in W_1+W_2$ can be expressed as a linear combination of vectors in the set of vectors \{$\vec{v_1}, \ldots, \vec{v_n}, \vec{u_1}, \ldots, \vec{u_m}$\}, i.e., $\vec{w} = c_1\vec{v_1} + \ldots + c_n\vec{v_n} + s_1\vec{u_1} + \ldots + s_m\vec{u_m}$, for some $c_1,\ldots,c_n,s_1,\ldots,s_m\in\mathbb{R}$. Hence, it suffices to say that the set of vectors \{$\vec{v_1}, \ldots, \vec{v_n}, \vec{u_1}, \ldots, \vec{u_m}$\} spans $W_1 + W_2$ \\

\noindent To show linear independence of the set of vectors \{$\vec{v_1}, \ldots, \vec{v_n}, \vec{u_1}, \ldots, \vec{u_m}$\}, it can be shown that the only linear combination of the vectors in the set that results in the zero vector ($\vec{0}^V$) is the trivial combination. This is verifiable because $W_1\cap W_2 = \{\vec{0}\}$, so it follows that if $c_1\vec{v_1} + \ldots + c_n\vec{v_n} + s_1\vec{u_1} + \ldots + s_m\vec{u_m} = \vec{0}^V$, then $c_1 = \ldots = c_n = s_1 = \ldots = s_m = 0$. This deduction can be made because the zero vector is not part of either basis, so thus we can conclude that the set of vectors \{$\vec{v_1}, \ldots, \vec{v_n}, \vec{u_1}, \ldots, \vec{u_m}$\}, is linearly independent. \\

\noindent Since both of the conditions for basis have been satisfied, that is that the set of vectors \{$\vec{v_1}, \ldots, \vec{v_n}, \vec{u_1}, \ldots, \vec{u_m}$\} is linearly independent, and that it is a spanning set of the subspace $W_1 + W_2$, then it can be said that the set of vectors is a basis. This then implies that the $\operatorname{dim}\left(W_1 + W_2\right) = \operatorname{dim}(W_1) + \operatorname{dim}(W_2)$ because the set of vectors that forms a basis of $W_1 + W_2$ is found by combining the set of vectors that form a basis for $W_1$ and the set of vectors that form a basis for $W_2$.
\newpage
\qs{}{Let
$$
W_1=\operatorname{span}\left\{\left[\begin{array}{l}
1 \\
0 \\
0
\end{array}\right],\left[\begin{array}{l}
0 \\
1 \\
0
\end{array}\right]\right\} \quad \text { and } \quad W_2=\operatorname{span}\left\{\left[\begin{array}{l}
0 \\
1 \\
0
\end{array}\right],\left[\begin{array}{l}
0 \\
0 \\
1
\end{array}\right]\right\}
$$
be subspaces of $\mathbb{R}^3$. \\
(a) Show that $W_1+W_2=\mathbb{R}^3$. \\
(b) What are:
$$
\operatorname{dim}\left(W_1+W_2\right), \quad \operatorname{dim}\left(W_1\right), \quad \operatorname{dim}\left(W_2\right), \quad \text { and } \quad \operatorname{dim}\left(W_1\right)+\operatorname{dim}\left(W_2\right) ?
$$
(c) Does $\operatorname{dim}\left(W_1+W_2\right)=\operatorname{dim}\left(W_1\right)+\operatorname{dim}\left(W_2\right)$ ? Does this contradict the previous problem? Why or why not?}
\sol \\
(a) In order to show that $W_1 + W_2 = \mathbb{R}^3$, it must be shown that $W_1 + W_2 \subseteq \mathbb{R}^3$, and it must also be shown that $\mathbb{R}^3 \subseteq W_1 + W_2$ \\
\noindent Firstly to show that $W_1 + W_2 \subseteq \mathbb{R}^3$
$$
\text{Let}\;\;\vec{v}=\begin{bmatrix} x \\ y \\ z \end{bmatrix}\in W_1 + W_2
$$
By definition of $W_1 + W_2$, there exists vectors $\vec{w_1}\in W_1$ and $\vec{w_2}\in W_2$ such that $\vec{v}=\vec{w_1}+\vec{w_2}$. Since $W_1$ and $W_2$ are subspaces of $\mathbb{R}^3$, then $\vec{w_1},\vec{w_2}\in\mathbb{R}^3$, so thus the sum of the vectors $\vec{v}$, is also a vector contained in $\mathbb{R}^3$. This implies that $W_1 + W_2 \subseteq \mathbb{R}^3$ \\

\noindent Next to show that $\mathbb{R}^3 \subseteq W_1 + W_2$
$$
\text{Let}\;\;\vec{v}=\begin{bmatrix} x \\ y \\ z \end{bmatrix} \in \mathbb{R}^3
$$
It must be shown that there exists some vectors $\vec{w_1}\in W_1$ and $\vec{w_2}\in W_2$ such that $\vec{v} = \vec{w_1} + \vec{w_2}$. From the definition of $W_1$ and $W_2$ we can choose vectors $\vec{w_1} = \begin{bmatrix} x \\ y \\ 0 \end{bmatrix}$ and $\vec{w_2} = \begin{bmatrix} 0 \\ 0 \\ z \end{bmatrix}$. Then
$$
\begin{aligned}
	\vec{w_1} + \vec{w_2} & = \begin{bmatrix} x \\ y \\ 0 \end{bmatrix} + \begin{bmatrix} 0 \\ 0 \\ z \end{bmatrix} \\
	& = \begin{bmatrix} x \\ y \\ z \end{bmatrix} \\
	& = \vec{v}
\end{aligned}
$$
So thus this shows that $\mathbb{R}^3\subseteq W_1 + W_2$. Since it has been shown that $\mathbb{R}^3\subseteq W_1 + W_2$ and that $W_1 + W_2 \subseteq \mathbb{R}^3$ this implies that $W_1 + W_2 = \mathbb{R}^3$. \\
\noindent (b)
$$
\begin{aligned}
\operatorname{dim}\left(W_1+W_2\right) & = 3 \\
\operatorname{dim}\left(W_1\right) & = 2 \\
\operatorname{dim}\left(W_2\right) & = 2 \\
\operatorname{dim}\left(W_1\right) + \operatorname{dim}\left(W_2\right) & = 4
\end{aligned}
$$
(c) In this example, $\operatorname{dim}\left(W_1+W_2\right) \neq \operatorname{dim}\left(W_1\right) + \operatorname{dim}\left(W_2\right)$. This is because the vector subspaces contain a common vector that is not the zero vector. They both span a 2 dimensional subspace of $\mathbb{R}^3$, but since $W_1 + W_2 = \mathbb{R}^3$, the dimension will match that of $\mathbb{R}^3$ which is 3.

\end{document}
