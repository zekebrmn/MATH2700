\documentclass{report}

\input{preamble}
\input{macros}
\input{letterfonts}


\title{\Huge{Math 2700.009}\\Exam 2 Notes}
\author{\huge{Ezekiel Berumen}}
\date{}

\begin{document}

\maketitle
\newpage% or \cleardoublepage
% \pdfbookmark[<level>]{<title>}{<dest>}
\pdfbookmark[section]{\contentsname}{toc}
\tableofcontents
\pagebreak

\section{Relationships between the dimension of a space and the possible sizes of linearly (in)dependent and spanning sets}
\dfn{Dimension}{The dimension of $V$ denoted $\operatorname{dim}(V)$ is the number of elements in a basis}
\noindent By the definition of dimension, we know that it represents the number of linearly independent elements that also span the space. That is to say, that the size of basis for a space cannot exceed the dimension of that space. As a result, you cannot have more linearly independent elements in a space than that of the size of the basis. Spanning sets however differ, since the span of a set of vectors can potentially contain linearly dependent vectors. In particular we say that: \\
\\
\noindent If $S\le V$ and $\vec{v}\in\operatorname{span}(S)$, $\vec{v}\in S$, then if $S'$ is obtained by removing $\vec{v}$ from $S$ and $\vec{v}\in\operatorname{span}(S')$, then $\operatorname{span}(S)=\operatorname{span}(S')$ \\
\\
\noindent In more simple terms, we can have two unique sets of vectors which can have equal spans. This is because the span of a linearly independent set of vectors is equal to that of a linearly dependent vectors which span the same space.
\section{How to find a basis for a subspace of $\mathbb{R}^n$ and the dimension of a subspace}
To find a basis of a subspace in $\mathbb{R}^n$,  we must ensure that the set of vectors selected meet certain conditions.
\begin{itemize}
\item The span of the set of vectors contains the zero vector from $\mathbb{R}^n$
\item The span of the set of vectors is closed under addition and scalar multiplication.
\end{itemize}
I will include examples from previous homeworks to demonstrate different methods for which this can be verified. 
\qs{Homework 05, Question 3}{What is the dimension of the following subspace of $\mathbb{R}^3$ ? Justify your answer.
$$
\operatorname{span}\left(\left[\begin{array}{c}
1 \\
-1 \\
2
\end{array}\right],\left[\begin{array}{l}
0 \\
3 \\
4
\end{array}\right],\left[\begin{array}{l}
2 \\
1 \\
8
\end{array}\right]\right)
$$}
\sol The dimension of the subspace generated by taking the span of the vectors is 2. This is because one of the vectors in the set is linearly dependent, in particular we can see that
$$
\begin{aligned}
	2 \begin{bmatrix} 1 \\ -1 \\ 2 \end{bmatrix} +
	\begin{bmatrix} 0 \\ 3 \\ 4 \end{bmatrix}  & =
	\begin{bmatrix} 2 \\ -2 \\ 4 \end{bmatrix} +
	\begin{bmatrix} 0 \\ 3 \\ 4 \end{bmatrix} \\
	& = \begin{bmatrix} 2 + 0 \\ -2 + 3 \\ 4 + 4 \end{bmatrix} \\
	& = \begin{bmatrix} 2 \\ 1 \\ 8 \ \end{bmatrix}
\end{aligned}
$$
Because of this, it is sufficient to say that the vectors form a 2-dimensional subspace of $\mathbb{R}^3$. This is because by definition, a basis is a set of linearly independent vectors that span a subspace. Since the two remaining vectors are linearly indepdendent and span the subspace, they serve as a basis for this subspace, and thus the dimension is 2. \\
\\
\noindent However,  this is not the only methodology to finding a basis and the dimension of a subspace from $\mathbb{R}^n$.  Below is a modified version of a previous homework question to demonstrate another method for finding a subspaces basis and its dimension.
\qs{Homework 05, Question 8}{Let
$$
W = \operatorname{span}\left\{\begin{bmatrix} 1\\0\\0 \end{bmatrix},\begin{bmatrix} 0\\1\\0 \end{bmatrix}\right\}
$$
be a subspace of $\mathbb{R}^3$\\
(a) Find a basis for $W$\\
(b) Find the $\dim(W)$}
\sol Since $W$ contains all linear combinations of the vectors
$$
\left\{\begin{bmatrix} 1\\0\\0 \end{bmatrix},\begin{bmatrix} 0\\1\\0 \end{bmatrix}\right\}
$$
Then we can use any scalar multiple of these to attempt to argue the case that they form a basis for the subspace $W$.  The intuitive option will be to use the vectors as they appear in the definition,  and that is what we will do.  To verify that they form a basis for the subspace,  then we can first check linear independence.  We can see that there is no way to generate a vector as a linear combination of the other so that condition is met. \\
\\
\noindent SInce the subspace definition tells us that the subspace is generated by taking the span of the two vectors,  then that automatically satisfies the spanning condition of a basis,  hence it is sufficient to say that the set of vectors forms a basis for the subspace $W$.\\
\\
\noindent We have just found a basis of $W$ to contain two vectors,  so if we appeal to the definition of dimension, then we can say that $\dim(W)=2$.  This is because,  as we saw earlier the number of vectors in a basis is what defines the dimension,  hence why the dimension of $W$ is equal to 2 when a basis for $W$ contains two elements.
\section{How to verify a function is a linear transformation}
Recall that if $V,W$ are vector spaces, to verify a linear transformation from $V$ to $W$, two properties must be verified:
\begin{itemize}
\item Additivity: A function $T:V\rightarrow W$ is additive for any $\vec{x},\vec{y}\in V$
$$
T(\vec{x}+\vec{y}) = T(\vec{x}) + T(\vec{y})
$$
\item Homogenity: A function $T:V \rightarrow W$ is homogenous if for any $c\in\mathbb{R}$ and $\vec{x}\in V$
$$
T(c\vec{x}) = cT(\vec{x})
$$
\end{itemize}
\pagebreak
\dfn{Kernel}{The kernel (sometimes known as null-sapce) of a linear transformation is the subset of the domain that is transformed into the zero vector}
\thm{Rank-Nullity}{If $T: V\rightarrow W$ is a linear transformation:
$$
\dim(V) = \dim(\ker(T)) + \dim(\ran(T))
$$}
\section{How to find the matrix of a linear transformation}
\textbf{$T:\mathbb{R}^n\rightarrow\mathbb{R}^k$ with the standard bases} \\
For every linear transformation $T:\mathbb{R}^n\rightarrow\mathbb{R}^k$ has an $A\in\mathbb{R}^{k\times n}$ such that
$$
[T(\vec{x})]_{\mathfrak{E}^k} = A[\vec{x}]_{\mathfrak{E}^n}
$$
To find such a matrix $A$ for the transformation,  a matrix can be constructed by taking the values of the transformed standard basis vectors and "glueing" them together,  in particular we say that
$$
A = \begin{bmatrix}
\vline&\vline&&\vline\\
&&&&\\
\quad T(e_1)&T(e_2)&\ldots&T(e_n)\\
&&&\\
\vline&\vline&&\vline
\end{bmatrix}
$$
\textbf{$T:W \rightarrow W$ when a basis for $W$ is given} \\
For every transformation $T:W \rightarrow W$ where $\dim(W)=n$ and a basis $\mathfrak{B}=\langle b_1, b_2, \ldots, b_n\rangle$,  there exists a matrix $A\in\mathbb{R}^{n\times n}$ such that
$$
[T(\vec{x})]_\mathfrak{B} = A[\vec{x}]_\mathfrak{B}
$$
To find such a matrix $A$ for the transformation,  a matrix can be constructed by computing the coordinates of the transformed basis vectors and "glueing" them together,  in particular we say that
$$
A = \begin{bmatrix}
\vline&\vline&&\vline\\
&&&&\\
\quad[T(b_1)]_\mathfrak{B}&[T(b_2)]_\mathfrak{B}&\ldots&[T(b_n)]_\mathfrak{B}\\
&&&\\
\vline&\vline&&\vline
\end{bmatrix}
$$
\subsection{How to find change of basis matrix}
If $\mathfrak{B}_1$ and $\mathfrak{B}_2$ are bases for $\mathbb{R}^n$,  then there exists a matrix $P_{\mathfrak{B}_1\rightarrow\mathfrak{B}_2}\in\mathbb{R}^{n\times n}$ so that for all $\vec{x}\in\mathbb{R}^n$
$$
P_{\mathfrak{B}_1\rightarrow\mathfrak{B}_2}[\vec{x}]_{\mathfrak{B}_1} = [\vec{x}]_{\mathfrak{B}_2}
$$
Notice if $\mathfrak{B}_3$ is another basis
$$
P_{\mathfrak{B}_2\rightarrow\mathfrak{B}_3}P_{\mathfrak{B}_1\rightarrow\mathfrak{B}_2}[\vec{x}]_{\mathfrak{B}_1} = [\vec{x}]_{\mathfrak{B}_3}
$$
In particular,  we also see that $(P_{\mathfrak{B}_1\rightarrow\mathfrak{B}_2})^{-1} = P_{\mathfrak{B}_2\rightarrow\mathfrak{B}_1}$. This is particularly useful when finding the matrix of a linear transformation with non-standard bases. \\
\\
\textbf{$T:\mathbb{R}^n\rightarrow\mathbb{R}^k$ with the non-standard bases} \\
For every linear transformation $T:\mathbb{R}^n\rightarrow\mathbb{R}^k$ with bases $\mathfrak{B}^n = \langle b_1, b_2,\ldots,b_n \rangle,\mathfrak{B}^k=\langle v_1,v_2,\ldots,v_k \rangle$,  there exists a matrix $A\in\mathbb{R}^{k\times n}$ such that
$$
[T(\vec{x})]_{\mathfrak{B}^k} = A[\vec{x}]_{\mathfrak{B}^n}
$$
Such a matrix can be constructed by applying the formula
$$
A = P_{\mathfrak{E}^k\rightarrow\mathfrak{B}^k}MP_{\mathfrak{B}^n\rightarrow\mathfrak{E}^n}
$$
Where M is the matrix from the original function definition of $T$ such that $T(\vec{x}) = M\vec{x}$
\ex{Matrix of $T:\mathbb{R}^3\rightarrow\mathbb{R}^2$}{
If $T:\mathbb{R}^3\rightarrow\mathbb{R}^2$ is a linear transformation with definition
$$
T(\vec{x}) = \begin{bmatrix} 1&2&-1\\0&2&1 \end{bmatrix}\vec{x}
$$
With ordered bases $\mathfrak{B}^3,\mathfrak{B}^2$
$$
\mathfrak{B}^3 = \left\langle \begin{bmatrix}1\\-1\\0\end{bmatrix},\begin{bmatrix}1\\0\\2\end{bmatrix},\begin{bmatrix}0\\3\\0\end{bmatrix} \right\rangle
$$
$$
\mathfrak{B}^2 = \left\langle \begin{bmatrix}1\\1\end{bmatrix},\begin{bmatrix}1\\0\end{bmatrix}\right\rangle
$$
To find our matrix $A$,  we can use the formula from earlier
$$
A = P_{\mathfrak{E}^2\rightarrow\mathfrak{B}^2}MP_{\mathfrak{B}^3\rightarrow\mathfrak{E}^3}
$$
Where $\mathfrak{E}^n$ are the standard bases.  We must compute the change of basis matrices for the ordered bases that we have.  In particular, we must compute $P_{\mathfrak{B}^3\rightarrow\mathfrak{E}^3}, P_{\mathfrak{B}^2\rightarrow\mathfrak{E}^2}$ and $(P_{\mathfrak{B}^2\rightarrow\mathfrak{E}^2})^{-1}$
$$
P_{\mathfrak{B}^3\rightarrow\mathfrak{E}^3} = \begin{bmatrix} 1&1&0\\-1&0&3\\0&2&0 \end{bmatrix}
$$
$$
P_{\mathfrak{B}^2\rightarrow\mathfrak{E}^2} = \begin{bmatrix} 1&1\\1&0 \end{bmatrix}
$$
$$
(P_{\mathfrak{B}^2\rightarrow\mathfrak{E}^2})^{-1} = -1\begin{bmatrix} 0&-1\\-1&1 \end{bmatrix}
$$
Now the matrix $A$ can be computed
$$
\begin{aligned}
A & = \begin{bmatrix} 0&1\\1&-1 \end{bmatrix}\begin{bmatrix} 1&2&-1\\0&2&1 \end{bmatrix}\begin{bmatrix} 1&1&0\\-1&0&3\\0&2&0 \end{bmatrix} \\
& =\begin{bmatrix} 0&1\\1&-1 \end{bmatrix}\begin{bmatrix} -1&-1&-6\\-2&2&6 \end{bmatrix} \\
& = \begin{bmatrix} -2&2&6\\1&-3&0 \end{bmatrix}
\end{aligned}
$$
And thus we have computed a matrix $A$ such so that $[T(\vec{x})]_{\mathfrak{B}^2}=A[\vec{x}]_{\mathfrak{B}^3}$
}
\section{How to find bases for the range and kernel of a matrix}
To find a basis for the range of a matrix,  we can take the linearly independent columns of a given matrix.  I will use an example from Quiz 06 to help demonstrate this.
$$
A = \begin{bmatrix}1&1&2\\0&1&1\\0&0&0\end{bmatrix}
$$
We can see that while the first two columns are linearly independent,  the third column can be expressed as some linear combination of the first two,  in particular
$$
\begin{bmatrix}2\\1\\0\end{bmatrix} = \begin{bmatrix}1\\0\\0\end{bmatrix} + \begin{bmatrix}1\\1\\0\end{bmatrix}
$$
We can then say that a basis for the range of $A$ is
$$
\left\{\begin{bmatrix}1\\0\\0\end{bmatrix},\begin{bmatrix}1\\1\\0\end{bmatrix}\right\}
$$
To construct a basis for the kernel of $A$,  we must analyze the linearly dependent vector.  We see that $\vec{a_3} = \vec{a_1} + \vec{a_2}$ where $\vec{a_j}$ is the $j$th column of $A$.  We can manipulate this equation to see that
$$
\vec{a_1}+\vec{a_2}-\vec{a_3} = \vec{0}
$$
This is sufficient to say that an element of the kernel of $A$ is $\begin{bmatrix}1&1&-1\end{bmatrix}^\top$.  The kernel will contain all scalar multiples of this vector,  however we only need one for a basis of the kernel so we can say that
$$
\left\{\begin{bmatrix}1\\1\\-1\end{bmatrix}\right\}
$$
is a basis for the $\ker(A)$
\dfn{Isomorphism}{If $T:V\rightarrow W$ is a linear transformation,  then if $T$ is bijective,  we call it an Isomorphism.}
\section{Finding full solution sets using kernel and change of basis}
A useful property of the change of basis matrices is the ability to find the coordinate for an ordered basis to the linear combination of vectors that results in a given solution.  I will use the example from Quiz 09 to express this technique. \\
\\
\noindent Given a matrix $A$
$$
A = \begin{bmatrix}
2&1&3\\
3&2&5\\
0&0&0
\end{bmatrix}
$$
We can find solutions for the following using a change of basis matrix.
$$
A\vec{x} = \begin{bmatrix}6/5\\19/10\\0\end{bmatrix}\qquad\qquad\text{and}\qquad\qquad A\vec{x}=\begin{bmatrix}1\\2\\4\end{bmatrix}
$$
Firstly,  we must find a basis for the range of $A$.  Since the third column is a linear combination of the first two we can say an ordered basis $\mathfrak{B}$ for $A$ is
$$
\mathfrak{B} = \langle\vec{a_1},\vec{a_2}\rangle
$$
In order to find a change of basis matrix,  we must expand the ordered basis so that it is an ordered basis for $\mathbb{R}^3$.  We can call this new ordered basis $\mathfrak{B}'$ defined by
$$
\mathfrak{B}' = \langle\vec{a_1},\vec{a_2}\vec{e_3}\rangle
$$
Where $\vec{e_3}$ is the 3rd standard basis vector for $\mathbb{R}^3$.  We can find $P_{\mathfrak{E}\rightarrow\mathfrak{B}'}$ to be
$$
\begin{bmatrix}
2&1&0\\
3&2&0\\
0&0&1
\end{bmatrix}^{-1} = \begin{bmatrix}
2&-1&0\\
-3&2&0\\
0&0&1
\end{bmatrix}
$$
This is particularly useful because the product $P_{\mathfrak{E}\rightarrow\mathfrak{B}'}$ and the value of $A\vec{x}$ will result in the $\mathfrak{B}'$-coordinate for a solution $\vec{x}$. 
$$
P_{\mathfrak{E}\rightarrow\mathfrak{B}'}\begin{bmatrix}6/5\\19/10\\0\end{bmatrix} =
\begin{bmatrix}
2&-1&0\\
-3&2&0\\
0&0&1
\end{bmatrix}\begin{bmatrix}6/5\\19/10\\0\end{bmatrix} =
\begin{bmatrix}1/2\\1/5\\0\end{bmatrix}
$$
This doesn't immediately tell us everything we need to know,  because recall this is a $\mathfrak{B}'$-coordinate solution to $\vec{x}$,  but the basis $\mathfrak{B}'$ was constructed using an additional standard basis vector.  In this case since the third coordinate is zero,  then that means the solution can exists without utilizing the standard basis vector we added earlier.  \\
\\
\noindent We can expand upon this further by using the kernel to form a full set of solutions.  We saw earlier that $\vec{a_1}+\vec{a_2}=\vec{a_3}$.  This means that $\vec{a_1}+\vec{a_2}-\vec{a_3} = 0$.  This means that a basis for the kernel is
$$
\left\{\begin{bmatrix}1\\1\\-1\end{bmatrix}\right\}
$$
So now we can expand the full set of solutions to be
$$
\left\{
\begin{bmatrix}1/2\\1/5\\0\end{bmatrix} + c_1\begin{bmatrix}1\\1\\-1\end{bmatrix} : c_1\in\mathbb{R}
\right\}
$$
A similar process can be used for the other vector
$$
P_{\mathfrak{E}\rightarrow\mathfrak{B}'}\begin{bmatrix}1\\2\\4\end{bmatrix} =
\begin{bmatrix}
2&-1&0\\
-3&2&0\\
0&0&1
\end{bmatrix}\begin{bmatrix}1\\2\\4\end{bmatrix} =
\begin{bmatrix}0\\1\\4\end{bmatrix}
$$
In this case,  we see that the third coordinate is nonzero.  This means that this solution was constructed using the additional standard basis vector we added earlier to make $\mathbb{B}'$.  This means that there does not exist a solution for this $\vec{x}$. 
\section{Multilinear Forms}
A multilinear form is a type of function which accepts multiple vectors as inputs and satisfies certain conditions. More specifically,  a linear $n$-form is a function $f:V^n\rightarrow\mathbb{R}$,  where $V$ is a vector space,  and it takes $n$ vectors as inputs. \\
\\
\noindent For example $f:V\times V\rightarrow\mathbb{R}$ a linear two form if it satisfies the conditions
\begin{itemize}
\item \textbf{Homogenity:}
$$
\left\{\begin{aligned}
f(c\vec{v},  \vec{w}) & = cf(\vec{v},\vec{w})\\
f(\vec{v},  c\vec{w}) & = cf(\vec{v},\vec{w})
\end{aligned}
\right.  \text{ where } c\in\mathbb{R} \text{ and } \vec{v},\vec{w}\in V
$$
\item \textbf{Additivity:}
$$
\left\{\begin{aligned}
f(\vec{v}+\vec{u}, \vec{w}) & = f(\vec{v},\vec{w})+f(\vec{u},\vec{w})\\
f(\vec{v},  \vec{w}+\vec{u}) & = f(\vec{v},\vec{w})+f(\vec{v},\vec{u})
\end{aligned}
\right.  \text{ where } \vec{v},\vec{w},\vec{u}\in V
$$
\end{itemize}
I've included an example from our homework to demonstrate properties of linear $n$-forms.
\qs{Homework 09,  Question 3}{
Suppose $f: \mathbb{R}^2 \times \mathbb{R}^2 \times \mathbb{R}^2 \rightarrow \mathbb{R}$ was a 3-linear form on $\mathbb{R}^2$ such that
$$
\begin{aligned}
& f\left(\begin{bmatrix}
1 \\
0
\end{bmatrix},\begin{bmatrix}
1 \\
0
\end{bmatrix},\begin{bmatrix}
1 \\
0
\end{bmatrix}\right)=0, \qquad  f\left(\begin{bmatrix}
1 \\
0
\end{bmatrix},\begin{bmatrix}
0 \\
1
\end{bmatrix},\begin{bmatrix}
1 \\
0
\end{bmatrix}\right)=1 \\
& f\left(\begin{bmatrix}
1 \\
0
\end{bmatrix},\begin{bmatrix}
1 \\
0
\end{bmatrix},\begin{bmatrix}
0 \\
1
\end{bmatrix}\right)=-1, \qquad  f\left(\begin{bmatrix}
1 \\
0
\end{bmatrix},\begin{bmatrix}
0 \\
1
\end{bmatrix},\begin{bmatrix}
0 \\
1
\end{bmatrix}\right)=1 \\
& f\left(\begin{bmatrix}
0 \\
1
\end{bmatrix},\begin{bmatrix}
1 \\
0
\end{bmatrix},\begin{bmatrix}
1 \\
0
\end{bmatrix}\right)=2, \qquad f\left(\begin{bmatrix}
0 \\
1
\end{bmatrix},\begin{bmatrix}
0 \\
1
\end{bmatrix},\begin{bmatrix}
1 \\
0
\end{bmatrix}\right)=2 \\
& f\left(\begin{bmatrix}
0 \\
1
\end{bmatrix},\begin{bmatrix}
1 \\
0
\end{bmatrix},\begin{bmatrix}
0 \\
1
\end{bmatrix}\right)=-3, \qquad f\left(\begin{bmatrix}
0 \\
1
\end{bmatrix},\begin{bmatrix}
0 \\
1
\end{bmatrix},\begin{bmatrix}
0 \\
1
\end{bmatrix}\right)=1.
\end{aligned}
$$

Compute
$$
f\left(\begin{bmatrix}
1 \\
2
\end{bmatrix},\begin{bmatrix}
3 \\
4
\end{bmatrix},\begin{bmatrix}
0 \\
-1
\end{bmatrix}\right) .
$$}
\sol To compute the value we can apply linearity properties of $f$, in particular, addition and scalar properties. Firstly
$$
\begin{aligned}
f\left(\begin{bmatrix} 1 \\ 2 \end{bmatrix}, \begin{bmatrix} 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \end{bmatrix} \right) & =
f\left(\begin{bmatrix} 1 \\ 0 \end{bmatrix} + \begin{bmatrix} 0 \\ 2 \end{bmatrix}, \begin{bmatrix} 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \end{bmatrix} \right) \\
& = f\left(\begin{bmatrix} 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \end{bmatrix} \right) +
f\left(\begin{bmatrix} 0 \\ 2 \end{bmatrix}, \begin{bmatrix} 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \end{bmatrix} \right) \\
& = f\left(\begin{bmatrix} 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \end{bmatrix} \right) +
2f\left(\begin{bmatrix} 0 \\ 1 \end{bmatrix}, \begin{bmatrix} 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \end{bmatrix} \right) \\
& = -1 + 2(-3) = -7
\end{aligned}
$$
We can use this calculated value for subsequent computations, next to modify the final $\mathbb{R}^2$ vector since it will take only one operation we can see that
$$
\begin{aligned}
f\left(\begin{bmatrix} 1 \\ 2 \end{bmatrix}, \begin{bmatrix} 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ -1 \end{bmatrix} \right) & = 
f\left(\begin{bmatrix} 1 \\ 2 \end{bmatrix}, \begin{bmatrix} 1 \\ 0 \end{bmatrix}, -1\begin{bmatrix} 0 \\ 1 \end{bmatrix} \right) \\
& = -f\left(\begin{bmatrix} 1 \\ 2 \end{bmatrix}, \begin{bmatrix} 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \end{bmatrix} \right) \\
& = 7
\end{aligned}
$$
And then lastly the middle vector:
$$
\begin{aligned}
f\left(\begin{bmatrix} 1 \\ 2 \end{bmatrix}, \begin{bmatrix} 3 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ -1 \end{bmatrix} \right) & =
f\left(\begin{bmatrix} 1 \\ 2 \end{bmatrix}, 3\begin{bmatrix} 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ -1 \end{bmatrix} \right) \\
& = 3f\left(\begin{bmatrix} 1 \\ 2 \end{bmatrix}, \begin{bmatrix} 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ -1 \end{bmatrix} \right) \\
& = 21
\end{aligned}
$$
However you may notice that this requires the calculation of another value of $f$ to continue, in particular we must find
$$
f\left(\begin{bmatrix} 1 \\ 2 \end{bmatrix}, \begin{bmatrix} 0 \\ 4 \end{bmatrix}, \begin{bmatrix} 0 \\ -1 \end{bmatrix} \right)
$$
To do so lets repeat similar steps
$$
\begin{aligned}
f\left(\begin{bmatrix} 1 \\ 2 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \end{bmatrix} \right) & =
f\left(\begin{bmatrix} 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \end{bmatrix} \right) + 2f\left(\begin{bmatrix} 0 \\ 1 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \end{bmatrix} \right) \\
& = 1 + 2(1) = 3
\end{aligned}
$$
Similar to before we must also find that
$$
\begin{aligned}
f\left(\begin{bmatrix} 1 \\ 2 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \end{bmatrix}, \begin{bmatrix} 0 \\ -1 \end{bmatrix} \right) & = 
-f\left(\begin{bmatrix} 1 \\ 2 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \end{bmatrix} \right) \\
& = -3
\end{aligned}
$$
And furthermore we must compute
$$
\begin{aligned}
f\left(\begin{bmatrix} 1 \\ 2 \end{bmatrix}, \begin{bmatrix} 0 \\ 4 \end{bmatrix}, \begin{bmatrix} 0 \\ -1 \end{bmatrix} \right) & =
4f\left(\begin{bmatrix} 1 \\ 2 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \end{bmatrix}, \begin{bmatrix} 0 \\ -1 \end{bmatrix} \right) \\
& = -12
\end{aligned}
$$
Since we now have all required pieces, we can put them together to compute the final unknown value and we see that
$$
\begin{aligned}
f\left(\begin{bmatrix} 1 \\ 2 \end{bmatrix}, \begin{bmatrix} 3 \\ 4 \end{bmatrix}, \begin{bmatrix} 0 \\ -1 \end{bmatrix} \right) & =
f\left(\begin{bmatrix} 1 \\ 2 \end{bmatrix}, \begin{bmatrix} 3 \\ 0 \end{bmatrix} + \begin{bmatrix} 0 \\ 4 \end{bmatrix}, \begin{bmatrix} 0 \\ -1 \end{bmatrix} \right) \\
& = f\left(\begin{bmatrix} 1 \\ 2 \end{bmatrix}, \begin{bmatrix} 3 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ -1 \end{bmatrix} \right) +
f\left(\begin{bmatrix} 1 \\ 2 \end{bmatrix}, \begin{bmatrix} 0 \\ 4 \end{bmatrix}, \begin{bmatrix} 0 \\ -1 \end{bmatrix} \right) \\
& = 21 - 12 = 9
\end{aligned}
$$
\section{Determinant}
\dfn{Determinant}{A determinant is an $n$-linear form on $\mathbb{R}^n$ with the properties:\\
$f:\mathbb{R}^{n\times n}\rightarrow\mathbb{R}$
\begin{itemize}
\item $f(I_n) = 1$
\item If two columns of $A\in\mathbb{R}^{n\times n}$ are the same,  then $f(A) = 0$
\item If you swap two columns of $A$ to get $B$,  then $-f(A) = f(B)$
\end{itemize}}


\end{document}
